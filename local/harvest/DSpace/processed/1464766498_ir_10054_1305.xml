<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd"><identifier>ir-10054-1305</identifier><datestamp>2011-12-27T05:47:34Z</datestamp><dc:title>Focused crawling: a new approach to topic-specific Web resource discovery</dc:title><dc:creator>CHAKRABARTI, SOUMEN</dc:creator><dc:creator>BERG, MARTIN VAN DEN</dc:creator><dc:creator>DOM, BYRON</dc:creator><dc:subject>data reduction</dc:subject><dc:subject>data structures</dc:subject><dc:subject>hypertext systems</dc:subject><dc:subject>search engines</dc:subject><dc:description>The rapid growth of the World-Wide Web poses unprecedented scaling challenges for general-purpose crawlers and search engines. In this paper we describe a new hypertext resource discovery system called a Focused Crawler. The goal of a focused crawler is to selectively seek out pages that are relevant to a pre-defined set of topics. The topics are specified not using keywords, but using exemplary documents. Rather than collecting and indexing all accessible Web documents to be able to answer all possible ad-hoc queries, a focused crawler analyzes its crawl boundary to find the links that are likely to be most relevant for the crawl, and avoids irrelevant regions of the Web. This leads to significant savings in hardware and network resources, and helps keep the crawl more up-to-date.

To achieve such goal-directed crawling, we designed two hypertext mining programs that guide our crawler: a classifier that evaluates the relevance of a hypertext document with respect to the focus topics, and a distiller that identifies hypertext nodes that are great access points to many relevant pages within a few links. We report on extensive focused-crawling experiments using several topics at different levels of specificity. Focused crawling acquires relevant pages steadily while standard crawling quickly loses its way, even though they are started from the same root set. Focused crawling is robust against large perturbations in the starting set of URLs. It discovers largely overlapping sets of resources in spite of these perturbations. It is also capable of exploring out and discovering valuable resources that are dozens of links away from the start set, while carefully pruning the millions of pages that may lie within this same radius. Our anecdotes suggest that focused crawling is very effective for building high-quality collections of Web documents on specific topics, using modest desktop hardware.</dc:description><dc:publisher>Elsevier</dc:publisher><dc:date>2009-05-08T02:37:19Z</dc:date><dc:date>2011-12-08T06:58:32Z</dc:date><dc:date>2011-12-26T13:01:53Z</dc:date><dc:date>2011-12-27T05:47:34Z</dc:date><dc:date>2009-05-08T02:37:19Z</dc:date><dc:date>2011-12-08T06:58:32Z</dc:date><dc:date>2011-12-26T13:01:53Z</dc:date><dc:date>2011-12-27T05:47:34Z</dc:date><dc:date>1999</dc:date><dc:type>Article</dc:type><dc:identifier>Computer Networks 31(11-16), 1623-1640</dc:identifier><dc:identifier>1389-1286</dc:identifier><dc:identifier>10.1016/S1389-1286(99)00052-3</dc:identifier><dc:identifier>http://hdl.handle.net/10054/1305</dc:identifier><dc:identifier>http://dspace.library.iitb.ac.in/xmlui/handle/10054/1305</dc:identifier><dc:language>en</dc:language></oai_dc:dc>